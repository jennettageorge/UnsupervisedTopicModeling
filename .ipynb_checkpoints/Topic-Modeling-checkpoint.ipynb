{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "- import the urls from csv file\n",
    "- use `requests` to check if the status code is 200\n",
    "- for all good urls, scrape the text using Beautiful soup\n",
    "- use basic NLP techniques to clean, tokenize, and stem the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect urls from text file containing all URL sources\n",
    "def collect_urls(text_file):\n",
    "    urls = []\n",
    "    with open(text_file) as file:\n",
    "        for line in file:\n",
    "            if \"http\" in line:\n",
    "                urls.append('http' + line.split('http')[-1][:-2].replace(' ',''))\n",
    "    return urls\n",
    "\n",
    "#Some urls do not exist or take too long to load and halt the program ,\n",
    "#so we implement a timer to skip bad urls and collect all urls that\n",
    "#return a 200 code\n",
    "@contextmanager\n",
    "def timeout(time):\n",
    "    # Register a function to raise a TimeoutError on the signal.\n",
    "    signal.signal(signal.SIGALRM, raise_timeout)\n",
    "    # Schedule the signal to be sent after ``time``.\n",
    "    signal.alarm(time)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    except TimeoutError:\n",
    "        pass\n",
    "    finally:\n",
    "        # Unregister the signal so it won't be triggered\n",
    "        # if the timeout is not reached.\n",
    "        signal.signal(signal.SIGALRM, signal.SIG_IGN)\n",
    "\n",
    "\n",
    "def raise_timeout(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "\n",
    "def check_status(url):\n",
    "    # Add a timeout block.\n",
    "    with timeout(5):\n",
    "        try:\n",
    "            html_page = requests.get(url)\n",
    "            if html_page.status_code == 200:\n",
    "                print(url)\n",
    "                return url\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the urls from the text file\n",
    "urls = collect_urls('list_of_articles.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.theaustralian.com.au/in-depth/terror/failed-bids-for-living-safe-together-funding-awarded-money/news-story/8f2998c8d5257b70d307f015f2188b58\n",
      "https://www.aclu.org/cases/aclu-v-department-homeland-security-foia-lawsuit-seeking-records-countering-violent-extremism\n",
      "https://www.aclu.org/coalition-letter-obama-administration-countering-violent-extremism-cve-program\n",
      "http://www.strokeassociation.org/STROKEORG/LifeAfterStroke/HealthyLivingAfterStroke/ManagingMedicines/Anti-Clotting-Agents-Explained_UCM_310452_Article.jsp#.Vj_Cxyvl3L8\n",
      "https://www.adl.org/news/press-releases/adl-report-white-supremacist-murders-more-than-doubled-in-2017\n",
      "https://www.adl.org/sites/default/files/documents/assets/pdf/combating-hate/Aryan-Circle-Report.pdf\n",
      "https://www.adl.org/sites/default/files/documents/assets/pdf/combating-hate/CR_4499_WhiteSupremacist-Report_web_vff.pdf\n",
      "http://www.aaiusa.org/countering_violent_extremism_cve\n",
      "http://www.aph.gov.au/About_Parliament/Parliamentary_Departments/Parliamentary_Library/pubs/rp/rp1415/Quick_Guides/Extremism\n",
      "https://www.aph.gov.au/About_Parliament/Parliamentary_Departments/Parliamentary_Library/pubs/BriefingBook45p/ViolentExtremism\n",
      "https://www.theguardian.com/uk-news/2016/feb/03/prevent-strategy-sowing-mistrust-fear-muslim-communities\n",
      "http://www.ocregister.com/articles/muslim-744939-communities-community.html\n",
      "https://www.fas.org/sgp/crs/homesec/R42553.pdf\n",
      "http://www.theguardian.com/world/shortcuts/2014/sep/21/islamic-state-isis-isil-daesh\n",
      "https://www.cbsnews.com/news/less-foreign-isis-recruits/\n",
      "http://www.charityandsecurity.org/background/The_Impact_of_Counterterrorism_Measures_on_Charities_and_Donors_After_9/11\n",
      "http://www.cbsnews.com/news/white-house-creating-new-counter-terrorism-task-force/\n",
      "https://www.fbi.gov/news/testimony/worldwide-threats-and-homeland-security-challenges\n",
      "http://www.prri.org/research/survey-nearly-half-of-americans-worried-that-they-or-their-family-will-be-a-victim-of-terrorism/\n",
      "https://www.reviewjournal.com/crime/shootings/sheriff-says-more-than-1100-rounds-fired-in-las-vegas-shooting/\n",
      "http://www.brookings.edu/~/media/research/files/papers/2014/07/30-gender-conflict-prevention-countering-violent-extremism-couture/women-cve-formatted-72914-couture-final2.pdf\n",
      "https://www.dst.defence.gov.au/sites/default/files/basic_pages/documents/counter-terrorism-white-paper.pdf\n",
      "https://obamawhitehouse.archives.gov/sites/default/files/empowering_local_partners.pdf\n",
      "https://www.dhs.gov/sites/default/files/publications/2016_strategic_implementation_plan_empowering_local_partners_prev.pdf\n",
      "https://leb.fbi.gov/2014/october/a-new-approach-to-countering-violent-extremism-sharing-expertise-and-empowering-local-communities\n",
      "https://www.fbi.gov/news/stories/2016-hate-crime-statistics\n",
      "https://www.fbi.gov/about-us/investigate/terrorism/terrorism-definition\n",
      "https://www.fbi.gov/investigate/violent-crime/gangs\n",
      "https://www.fema.gov/nonprofit-security-grant-program\n",
      "https://www.brennancenter.org/analysis/stigmatizing-boston-muslim-community-no-way-build-trust\n",
      "http://www.nytimes.com/2015/11/02/us/fbi-tool-to-identify-extremists-is-criticized.html?smid=nytcore-iphone-share&smprod=nytcore-iphone&_r=1\n",
      "https://news.law.fordham.edu/wp-content/uploads/2017/09/TheAmericanException9-17.pdf\n",
      "https://www.theguardian.com/politics/2016/sep/29/anti-radicalisation-strategy-lacks-evidence-base-in-science\n",
      "http://www.abc.net.au/news/2017-07-11/islamic-national-curriculum-push-to-apply-faith-for-australia/8696330\n",
      "http://www.cnn.com/2017/07/01/politics/cve-funding-changes/index.html\n",
      "https://aic.gov.au/publications/tandi/tandi491\n",
      "http://www.chicagotribune.com/news/nationworld/ct-james-fields-jr-charlottesville-20170818-story.html\n",
      "http://www.npr.org/sections/thetwo-way/2016/12/15/505723552/jury-finds-dylann-roof-guilty-in-s-c-church-shooting\n",
      "https://www.fbi.gov/news/pressrel/press-releases/the-fbi-releases-final-report-of-the-9-11-review-commission\n",
      "http://www.icnl.org/research/library/files/Transnational/CVEUSIP.pdf\n",
      "http://www.publications.parliament.uk/pa/cm200910/cmselect/cmcomloc/65/65.pdf\n",
      "https://www.hrw.org/world-report/2015/country-chapters/saudi-arabia\n",
      "http://securitydata.newamerica.net/extremists/deadly-attacks.html\n",
      "https://www.start.umd.edu/pubs/PIRUS%20Research%20Brief_Jan%202015.pdf\n",
      "https://www.cnn.com/2018/04/13/politics/kfile-hhs-official-obama-clinton-treason/index.html\n",
      "http://foreignpolicy.com/2015/06/01/you-cant-fight-what-you-dont-understand-violent-extremism-islamic-state/\n",
      "http://www.crikey.com.au/2014/09/04/the-real-threat-of-terrorism-to-australians-by-the-numbers/\n",
      "http://www.wired.com/2012/04/language-and-bias/\n",
      "http://www.pewresearch.org/fact-tank/2016/11/21/anti-muslim-assaults-reach-911-era-levels-fbi-data-show/\n",
      "http://www.pbs.org/kenburns/prohibition/unintended-consequences/\n",
      "https://www.nytimes.com/2016/11/15/us/politics/fbi-hate-crimes-muslims.html?_r=0\n",
      "http://www.foxnews.com/us/2017/11/29/trump-retweets-videos-critical-muslims.html\n",
      "http://journalistsresource.org/studies/government/criminal-justice/terrorism-sentencing-prison-crime\n",
      "http://www.slate.com/blogs/the_slatest/2015/06/18/white_extremist_murders_killed_at_least_60_in_u_s_since_1995.html\n",
      "http://abcnews.go.com/International/officials-inadequate-government-efforts-prevent-us-terrorist-radicalization/story?id=32472382\n",
      "http://www.denverpost.com/2017/02/01/shooting-rtd-union-station-soldier-police-islam/\n",
      "https://www.nytimes.com/2017/11/05/us/church-shooting-texas.html\n",
      "http://doi.org/10.1080/15298868.2012.702425\n",
      "https://docs.google.com/forms/d/1mvnh8yzKxOwUG5C69hN6ccZlXhRmX9kUd4xdjWlNh0Y/viewform?c=0&w=1\n"
     ]
    }
   ],
   "source": [
    "#from the list of urls, collect good urls\n",
    "urls_to_clean = []\n",
    "for url in urls:\n",
    "    if check_status(url):\n",
    "        urls_to_clean.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list of good urls to a file\n",
    "urls_df = pd.DataFrame(urls_to_clean)\n",
    "urls_df.to_csv('good_urls_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in analyzing the topics of an article, so we will create a list of articles that consist of tokens of stemmed words, using the `NLTK` stemming tool. We will remove stop words with `SpaCy`'s stopword collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "articles =[]\n",
    "i=0\n",
    "for url in urls_to_clean:    \n",
    "    try:\n",
    "        html_page = requests.get(url)\n",
    "        soup = BeautifulSoup(html_page.text, 'html.parser')\n",
    "        article_text = []\n",
    "        for x in soup.find_all('p'):\n",
    "            txt = x.get_text()\n",
    "            if txt:\n",
    "                txt = re.sub(r\"[^a-zA-Z0-9]\", \" \", txt.lower())\n",
    "                words = txt.split() #words withhin a paragraph\n",
    "                words = [w for w in words if w not in list(STOP_WORDS)] # Remove stopwords\n",
    "                words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "                for word in words:\n",
    "                    article_text.append(word)\n",
    "        articles.append(article_text)\n",
    "    except:\n",
    "        print('{} was a bad article'.format(url))\n",
    "    if i%10==0:\n",
    "        print(i,'th url parsed')\n",
    "    i+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for article in articles:\n",
    "    if len(article) < 4:\n",
    "        articles.remove(article)\n",
    "        i+=1\n",
    "print('Removed {} redundant articles, we now have {} articles to model.'.format(i,len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bigrams for Topic Modeling\n",
    "\n",
    "Phrases and bigrams are helpful NLP tools offered in the `gensim` library that aid in topic modeling. For a given list of topics, we can feed it to a a Phraser and it will return word phrases (for example, the words \"law\" and \"enforcement\" are typically part of a phrase, so the phraser will return a new word \"law_enforcement\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "phrases = Phrases(articles, min_count=1, threshold=2)\n",
    "bigram = Phraser(phrases)\n",
    "#here's an example of a bigram thhat we capture from the 4th article\n",
    "bigram[articles[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets collect all of our bigrams into a list for each article:\n",
    "\n",
    "bigram_articles = []\n",
    "for article in bigram[articles]:\n",
    "    bigram_articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
